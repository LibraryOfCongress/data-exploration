{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmpeXx0L-g1j"
   },
   "source": [
    "# Downloading Search Results from Chronicling America\n",
    "\n",
    "Downloading search results is slightly different from downloading newspaper titles and batches. The steps in this example are modified so that the user will only download the pages of the search results. Furthermore, this method works specifically for newspapers. Other formats on loc.gov may behave differently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgQqC4i9-jmt"
   },
   "source": [
    "## Importing Modules [Required]\n",
    "The following imports are required for the scripts to run properly:\n",
    "\n",
    "---\n",
    "1. Run the following code below.\n",
    "    * It will import all the modules you need for this notebook.\n",
    "    * Do not change anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YUzFnkcAt9Zy"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNWxNhlQ-mno"
   },
   "source": [
    "## Define your API Search Query and Save Location [Required]\n",
    "After running the *Importing Modules* code (above),\n",
    "1. Paste your Search Query URL below, into the `searchURL = '{URL}'`\n",
    "2. Edit the file type you wish to download in `fileExtension = '{filetype}'`. PDF works best. But options Include:\n",
    "     * pdf\n",
    "     * jp2\n",
    "     * Note: If you wish to download the jpg version of the files, we recommend you follow the IIIF example at: https://github.com/LibraryOfCongress/data-exploration/tree/master/loc.gov%20IIIF%20API.\n",
    "3. Add the location where you want your files saved to in \"saveTo\"\n",
    "4. When ready, Run the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GiCLMfUwvjqL"
   },
   "outputs": [],
   "source": [
    "# Perform Query - Paste your API Search Query URL into the searchURL\n",
    "searchURL = 'https://www.loc.gov/collections/chronicling-america/?dl=page&end_date=1922-12-31&ops=PHRASE&qs=clara+bow&searchType=advanced&start_date=1922-12-01&fo=json'\n",
    "\n",
    "# Add your desired file type (extension). Options Include: pdf, jpeg, and xml (OCR files)\n",
    "fileExtension = 'pdf'\n",
    "\n",
    "# Add your Local saveTo Location\n",
    "saveTo = 'output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrDcAEga-nt6"
   },
   "source": [
    "## Run Functions and Limits [Required]\n",
    "Functions and limits define what will be included and excluded in the search for downloads.\n",
    "The code below will only download the newspaper pages from your search result. It will not download the whole newspaper issue.\n",
    "\n",
    "---\n",
    "1. Run the code below.\n",
    "    * Do not change anything.\n",
    "2. When the script is complete, it will tell you how many Newspaper Pages it found from your search.\n",
    "3. If you are satisfied with the amount of results, proceed to the next section to run the download.\n",
    "4. If you are not satisfied with the amount of results, go back and redo the API Search Query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28400,
     "status": "ok",
     "timestamp": 1711558097130,
     "user": {
      "displayName": "Mike Saelee",
      "userId": "09356275809855701196"
     },
     "user_tz": 240
    },
    "id": "tUztyM7xuF6F",
    "outputId": "72bf09d0-143b-45d9-a7b6-84b729ea2598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Success. Your API Search Query found 3 related newspaper pages. You may now continue.\n"
     ]
    }
   ],
   "source": [
    "'''Run P1 search and get a list of results.'''\n",
    "def get_item_ids(url, items=[], conditional='True'):\n",
    "    # Check that the query URL is not an item or resource link.\n",
    "    exclude = [\"loc.gov/item\",\"loc.gov/resource\"]\n",
    "    if any(string in url for string in exclude):\n",
    "        raise NameError('Your URL points directly to an item or '\n",
    "                        'resource page (you can tell because \"item\" '\n",
    "                        'or \"resource\" is in the URL). Please use '\n",
    "                        'a search URL instead. For example, instead '\n",
    "                        'of \\\"https://www.loc.gov/item/2009581123/\\\", '\n",
    "                        'try \\\"https://www.loc.gov/maps/?q=2009581123\\\". ')\n",
    "\n",
    "    # request pages of 100 results at a time\n",
    "    params = {\"fo\": \"json\", \"c\": 100, \"at\": \"results,pagination\"}\n",
    "    call = requests.get(url, params=params)\n",
    "    # Check that the API request was successful\n",
    "    if (call.status_code==200) & ('json' in call.headers.get('content-type')):\n",
    "        data = call.json()\n",
    "        results = data['results']\n",
    "        for result in results:\n",
    "            # Filter out anything that's a colletion or web page\n",
    "            filter_out = (\"collection\" in result.get(\"original_format\")) \\\n",
    "                    or (\"web page\" in result.get(\"original_format\")) \\\n",
    "                    or (eval(conditional)==False)\n",
    "            if not filter_out:\n",
    "                # Get the link to the item record\n",
    "                if result.get(\"id\"):\n",
    "                    item = result.get(\"id\")\n",
    "                    # Filter out links to Catalog or other platforms\n",
    "                    if item.startswith(\"http://www.loc.gov/resource\"):\n",
    "                      resource = item  # Assign item to resource\n",
    "                      items.append(resource)\n",
    "                    if item.startswith(\"http://www.loc.gov/item\"):\n",
    "                        items.append(item)\n",
    "        # Repeat the loop on the next page, unless we're on the last page.\n",
    "        if data[\"pagination\"][\"next\"] is not None:\n",
    "            next_url = data[\"pagination\"][\"next\"]\n",
    "            get_item_ids(next_url, items, conditional)\n",
    "\n",
    "        return items\n",
    "    else:\n",
    "            print('There was a problem. Try running the cell again, or check your searchURL.')\n",
    "\n",
    "# Create ids_list based on searchURL results\n",
    "ids_list = get_item_ids(searchURL, items=[])\n",
    "\n",
    "# prompt: add 'fo=json' to the end of each row in ids_list\n",
    "\n",
    "new_ids = []\n",
    "for id in ids_list:\n",
    "  if not id.endswith('&fo=json'):\n",
    "    id += '&fo=json'\n",
    "  new_ids.append(id)\n",
    "ids = new_ids\n",
    "\n",
    "print('\\nSuccess. Your API Search Query found '+str(len(new_ids))+' related newspaper pages. You may now continue.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlkK3U7V-sgY"
   },
   "source": [
    "## Download Files\n",
    "If you want to download the found items, follow the instructions below.\n",
    "\n",
    "---\n",
    "1. Run the code below.\n",
    "    * Do not change anything.\n",
    "2. When the script is complete, the downloads will be found in your \"saveTo\" location.\n",
    "3. A list of downloaded files will also be present on the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9572,
     "status": "ok",
     "timestamp": 1711558143798,
     "user": {
      "displayName": "Mike Saelee",
      "userId": "09356275809855701196"
     },
     "user_tz": 240
    },
    "id": "ohMTq3DC2OVL",
    "outputId": "f4dc301e-6348-4ac8-cc53-f314993f8d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 Downloaded Files\n",
      "https://tile.loc.gov/storage-services/service/ndnp/wyu/batch_wyu_ellison_ver01/data/sn92066979/0051701011A/1922122701/0427.pdf\n",
      "https://tile.loc.gov/storage-services/service/ndnp/dlc/batch_dlc_dalek_ver01/data/sn83045462/00280657232/1922122601/0584.pdf\n",
      "https://tile.loc.gov/storage-services/service/ndnp/uuml/batch_uuml_kloeden_ver01/data/sn85058393/print/1922121701/1327.pdf\n",
      "\n",
      "Success! Please check your saveTo location to see the saved files. You can also redownload the selected files using the links above.\n"
     ]
    }
   ],
   "source": [
    "print('\\n'+str(len(new_ids))+' Downloaded Files')\n",
    "\n",
    "# prompt: print page_url if it matches the fileExtension\n",
    "\n",
    "for item in new_ids:\n",
    "    call = requests.get(item)\n",
    "    if call.status_code == 200:\n",
    "        data = call.json()\n",
    "        page = data['page']\n",
    "        for page in page:\n",
    "            if 'url' in page:\n",
    "                page_url = page['url']\n",
    "                if page_url.endswith(fileExtension):\n",
    "                    print(page_url)\n",
    "\n",
    "# Get the page URLs\n",
    "page_urls = []\n",
    "for item in new_ids:\n",
    "    call = requests.get(item)\n",
    "    if call.status_code == 200:\n",
    "        data = call.json()\n",
    "        page = data['page']\n",
    "        for page in page:\n",
    "            if 'url' in page:\n",
    "                page_url = page['url']\n",
    "                if page_url.endswith(fileExtension):\n",
    "                    page_urls.append(page_url)\n",
    "\n",
    "\n",
    "# prompt: create the folder and subfolder if they don't exist\n",
    "\n",
    "for page_url in page_urls:\n",
    "    # Extract the folder and filename from the URL\n",
    "    batch_name = page_url.split('/')[-6]\n",
    "    lccn_name = page_url.split('/')[-4]\n",
    "    reel_name = page_url.split('/')[-3]\n",
    "    issue_name = page_url.split('/')[-2]\n",
    "    filename = page_url.split('/')[-1]\n",
    "\n",
    "    # Create the batch folder if it doesn't exist\n",
    "    batch_path = os.path.join(saveTo, batch_name, lccn_name)\n",
    "    if not os.path.exists(batch_path):\n",
    "        os.makedirs(batch_path)\n",
    "\n",
    "    # Create the lccn folder if it doesn't exist\n",
    "    lccn_path = os.path.join(saveTo, batch_name, lccn_name)\n",
    "    if not os.path.exists(lccn_path):\n",
    "        os.makedirs(lccn_path)\n",
    "\n",
    "    # Create the reel folder if it doesn't exist\n",
    "    reel_path = os.path.join(saveTo, batch_name, lccn_name, reel_name)\n",
    "    if not os.path.exists(reel_path):\n",
    "        os.makedirs(reel_path)\n",
    "\n",
    "    # Create the issue subfolder if it doesn't exist\n",
    "    issue_path = os.path.join(saveTo, batch_name, lccn_name, reel_name, issue_name)\n",
    "    if not os.path.exists(issue_path):\n",
    "        os.makedirs(issue_path)\n",
    "\n",
    "    # Download the file\n",
    "    response = requests.get(page_url)\n",
    "    file_path = os.path.join(saveTo, batch_name, lccn_name, reel_name, issue_name, filename)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "print('\\nSuccess! Please check your saveTo location to see the saved files. You can also redownload the selected files using the links above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_ZP0Egt-v0C"
   },
   "source": [
    "## Get Basic Metadata/Information for your Downloaded Results\n",
    "\n",
    "\n",
    "If you need metadata/information for your downloads, run the script below\n",
    "The JSON parameters in the script can be changed per your requirements.\n",
    "\n",
    "---\n",
    "1. Run the code below.\n",
    "2. When the script is complete, a preview will be shown on the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 666,
     "status": "ok",
     "timestamp": 1711510861876,
     "user": {
      "displayName": "Mike Saelee",
      "userId": "09356275809855701196"
     },
     "user_tz": 240
    },
    "id": "sPJfESM22U-M",
    "outputId": "21c8fcab-c5ba-4c6d-ae6b-4cca10a8a461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Newspaper Title  Issue Date  Page Number          LCCN  \\\n",
      "0        [The Laramie Republican]  1922-12-27            5  [sn92066979]   \n",
      "1                 [Evening star.]  1922-12-26           22  [sn83045462]   \n",
      "2  [The Ogden standard-examiner.]  1922-12-17           12  [sn85058393]   \n",
      "\n",
      "           City                   State  \\\n",
      "0     [laramie]               [wyoming]   \n",
      "1  [washington]  [district of columbia]   \n",
      "2       [ogden]                  [utah]   \n",
      "\n",
      "                              Contributor                 Batch  \\\n",
      "0       [University of Wyoming Libraries]   [wyu_ellison_ver01]   \n",
      "1   [Library of Congress, Washington, DC]     [dlc_dalek_ver01]   \n",
      "2  [University of Utah, Marriott Library]  [uuml_kloeden_ver01]   \n",
      "\n",
      "                                            PDF Link  \n",
      "0  https://tile.loc.gov/storage-services/service/...  \n",
      "1  https://tile.loc.gov/storage-services/service/...  \n",
      "2  https://tile.loc.gov/storage-services/service/...  \n"
     ]
    }
   ],
   "source": [
    "# Create a list of dictionaries to store the item metadata\n",
    "item_metadata_list = []\n",
    "# Iterate over the list of item IDs\n",
    "for item in new_ids:\n",
    "# Make the API call to get the item metadata\n",
    "  item = requests.get(item)\n",
    "# Check if the API call was successful and Parse the JSON response\n",
    "  if item.status_code == 200:\n",
    "    new_ids_json = item.json()\n",
    "\n",
    "  # Extract the relevant item metadata\n",
    "  Newspaper_Title = new_ids_json['item']['newspaper_title']\n",
    "  Issue_Date = new_ids_json['item']['date']\n",
    "  Page = new_ids_json['pagination']['current']\n",
    "  State = new_ids_json['item']['location_state']\n",
    "  City = new_ids_json['item']['location_city']\n",
    "  LCCN = new_ids_json['item']['number_lccn']\n",
    "  Contributor = new_ids_json['item']['contributor_names']\n",
    "  Batch = new_ids_json['item']['batch']\n",
    "  pdf = new_ids_json['resource']['pdf']\n",
    "\n",
    "  # Add the item metadata to the list\n",
    "  item_metadata_list.append({\n",
    "    'Newspaper Title': Newspaper_Title,\n",
    "    'Issue Date': Issue_Date,\n",
    "    'Page Number': Page,\n",
    "    'LCCN': LCCN,\n",
    "    'City': City,\n",
    "    'State': State,\n",
    "    'Contributor': Contributor,\n",
    "    'Batch': Batch,\n",
    "    'PDF Link': pdf,\n",
    "    })\n",
    "\n",
    "# Create a Pandas DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(item_metadata_list)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C52LCoi9_GGC"
   },
   "source": [
    "## Export Metadata of Downloads to a CSV File.\n",
    "Before running the code, change **MetadataFileName** to the desired file name.\n",
    "\n",
    "---\n",
    "1. Run the code below.\n",
    "2. When the script is complete, the downloads will be found in your \"saveTo\" download location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1711511630716,
     "user": {
      "displayName": "Mike Saelee",
      "userId": "09356275809855701196"
     },
     "user_tz": 240
    },
    "id": "8rtcO2lF2X9r",
    "outputId": "47ee6b89-b6c3-4a5f-d923-6e0cc5ff15a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Success! Please check your saveTo location to see the saved csv file. See Preview Below:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Newspaper Title</th>\n",
       "      <th>Issue Date</th>\n",
       "      <th>Page Number</th>\n",
       "      <th>LCCN</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Contributor</th>\n",
       "      <th>Batch</th>\n",
       "      <th>PDF Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[The Laramie Republican]</td>\n",
       "      <td>1922-12-27</td>\n",
       "      <td>5</td>\n",
       "      <td>[sn92066979]</td>\n",
       "      <td>[laramie]</td>\n",
       "      <td>[wyoming]</td>\n",
       "      <td>[University of Wyoming Libraries]</td>\n",
       "      <td>[wyu_ellison_ver01]</td>\n",
       "      <td>https://tile.loc.gov/storage-services/service/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Evening star.]</td>\n",
       "      <td>1922-12-26</td>\n",
       "      <td>22</td>\n",
       "      <td>[sn83045462]</td>\n",
       "      <td>[washington]</td>\n",
       "      <td>[district of columbia]</td>\n",
       "      <td>[Library of Congress, Washington, DC]</td>\n",
       "      <td>[dlc_dalek_ver01]</td>\n",
       "      <td>https://tile.loc.gov/storage-services/service/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The Ogden standard-examiner.]</td>\n",
       "      <td>1922-12-17</td>\n",
       "      <td>12</td>\n",
       "      <td>[sn85058393]</td>\n",
       "      <td>[ogden]</td>\n",
       "      <td>[utah]</td>\n",
       "      <td>[University of Utah, Marriott Library]</td>\n",
       "      <td>[uuml_kloeden_ver01]</td>\n",
       "      <td>https://tile.loc.gov/storage-services/service/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Newspaper Title  Issue Date  Page Number          LCCN  \\\n",
       "0        [The Laramie Republican]  1922-12-27            5  [sn92066979]   \n",
       "1                 [Evening star.]  1922-12-26           22  [sn83045462]   \n",
       "2  [The Ogden standard-examiner.]  1922-12-17           12  [sn85058393]   \n",
       "\n",
       "           City                   State  \\\n",
       "0     [laramie]               [wyoming]   \n",
       "1  [washington]  [district of columbia]   \n",
       "2       [ogden]                  [utah]   \n",
       "\n",
       "                              Contributor                 Batch  \\\n",
       "0       [University of Wyoming Libraries]   [wyu_ellison_ver01]   \n",
       "1   [Library of Congress, Washington, DC]     [dlc_dalek_ver01]   \n",
       "2  [University of Utah, Marriott Library]  [uuml_kloeden_ver01]   \n",
       "\n",
       "                                            PDF Link  \n",
       "0  https://tile.loc.gov/storage-services/service/...  \n",
       "1  https://tile.loc.gov/storage-services/service/...  \n",
       "2  https://tile.loc.gov/storage-services/service/...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set File Name. Make sure to rename the file so it doesn't overwrite previous!\n",
    "filename = 'MetadataFileName'\n",
    "\n",
    "print('\\nSuccess! Please check your saveTo location to see the saved csv file. See Preview Below:\\n')\n",
    "\n",
    "metadata_dataframe = pd.DataFrame(item_metadata_list)\n",
    "metadata_dataframe.to_csv(saveTo + '/' + filename + '.csv')\n",
    "metadata_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNsHLgKqB6/7L6yA7K9G1bk",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
